{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from model.lightgbm_model import LightGbmModel\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from config import Config\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "import neologdn\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import MeCab\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#chaneg dummies to matrix\n",
    "from scipy.sparse import csr_matrix,coo_matrix\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import  roc_auc_score,f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate,Conv1D, MaxPool1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau,Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import os\n",
    "import tarfile\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"hoge progress: \")\n",
    "\n",
    "import time\n",
    "import gensim\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import Config\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlinking sudachidict\r\n",
      "sudachidict unlinked\r\n",
      "default dict package = sudachidict_full\r\n"
     ]
    }
   ],
   "source": [
    "!sudachipy link -t full\n",
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "def wakati_by_sudachi(txt):\n",
    "    processed=[m.normalized_form() for m in tokenizer_obj.tokenize(txt, mode)]\n",
    "    return \" \".join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(row):\n",
    "    cust_list=['\\d+','■','\\n','#','◇','①','②','③','④','【】','】','【','☆','_','%','「','」','★','/']\n",
    "    del_list = string.ascii_letters+'\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "    for i in del_list:\n",
    "        row = row.str.replace(i,'')\n",
    "    for i in cust_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    mapping ={\"矢張り\":\"やはり\",\"迚も\":\"とても\",\"迚も\":\"とても\",\"此れ\":\"これ\",\"其れ\":\"それ\",\"此の\":\"この\",\"此の\":\"この\",\"可成\":\"かなり\",\"兎に角\":\"とにかく\",\"態々\":\"わざわざ\"\n",
    "             ,\"です\":\"\",\"だ\":\"\",\"ます\":\"\",\"て\":\"\",\"た\":\"\"}\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment_data():\n",
    "    return pd.read_csv('/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Sentiment_Analyzer/raw_data/sum_data_for_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(pred, label):\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for idx, p in enumerate(pred):\n",
    "        total += 1\n",
    "        flag = np.argmax(p)\n",
    "        if int(flag) == int(label[idx]):\n",
    "            right += 1\n",
    "    return right / total\n",
    "\n",
    "def cal_f_alpha(pred, label, alpha=1.0, n_out=45, verbose=False):\n",
    "    # pred:  (x, 45)\n",
    "    # label: (x, 1)\n",
    "    # matrix = np.zeros((n_out, n_out))\n",
    "    matrix = np.diag(np.array([1e-7] * n_out))\n",
    "    for idx, p in enumerate(pred):\n",
    "        true_label = int(label[idx])\n",
    "        p = int(np.argmax(p))\n",
    "        if p == true_label:\n",
    "            matrix[p][p] += 1\n",
    "        else:\n",
    "            matrix[true_label][p] += 1\n",
    "\n",
    "    pi = []\n",
    "    ri = []\n",
    "    for i in range(n_out):\n",
    "        pi.append(matrix[i][i] / sum(matrix[:, i]) / n_out)\n",
    "        ri.append(matrix[i][i] / sum(matrix[i, :]) / n_out)\n",
    "\n",
    "    p = sum(pi)\n",
    "    r = sum(ri)\n",
    "    f = (alpha**2 + 1) * p * r / (alpha ** 2 * p + r)\n",
    "    if verbose:\n",
    "        # check every categories' prediction and recall\n",
    "        pass\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=load_comment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['comment'] = remove_chars(df_mutilabel['comment'])\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel.dropna(inplace=True)\n",
    "df_mutilabel['comment']=df_mutilabel['comment'].apply(neologdn.normalize)\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(wakati_by_sudachi,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "\n",
    "\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(clean_contractions,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel.dropna(inplace=True)\n",
    "df_mutilabel.fillna(' ',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df_mutilabel)) < 0.8\n",
    "train=df_mutilabel[msk]\n",
    "test=df_mutilabel[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,wins), min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "kfold_x_train = train[column]\n",
    "k_trn_term_doc = vec.fit_transform(kfold_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "n_folds = 5\n",
    "n_class = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=10)\n",
    "column = \"comment\"\n",
    "# column = \"article\"\n",
    "oof_predict = np.zeros((len(train[column]), n_class))\n",
    "predict = np.zeros((len(test[column]), n_class))\n",
    "cur_kfold = 0\n",
    "accs = []\n",
    "f1s = []\n",
    "wins = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total acc scores is  0.9443220240413062\n",
      "total f1 scores is  0.8680432984067095\n"
     ]
    }
   ],
   "source": [
    "#with LogisticRegression\n",
    "for train_index, test_index in kf.split(train[column]):\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    kfold_x_train = train[column][train_index]\n",
    "    kfold_x_valid = train[column][test_index]\n",
    "    k_y_train = (train['label']).astype(int)[train_index]\n",
    "    k_y_valid = (train['label']).astype(int)[test_index]\n",
    "\n",
    "    #print('获得tfidf特征')\n",
    "    k_trn_term_doc = vec.fit_transform(kfold_x_train)\n",
    "    k_test_term_doc = vec.transform(kfold_x_valid)\n",
    "    test_term_doc = vec.transform(test[column])\n",
    "\n",
    "    # 拟合数据\n",
    "    #print('拟合数据')\n",
    "    lin_clf = LogisticRegression(C=4, dual=True)\n",
    "    lin_clf.fit(k_trn_term_doc, k_y_train)\n",
    "\n",
    "    # 预测结果\n",
    "    #print('预测结果')\n",
    "    oof_predict[test_index] = lin_clf.predict_proba(k_test_term_doc)\n",
    "    predict += lin_clf.predict_proba(test_term_doc) / n_folds\n",
    "\n",
    "    # 计算准确度\n",
    "    accuracy = cal_acc(oof_predict[test_index], k_y_valid.values)\n",
    "    f1 = cal_f_alpha(oof_predict[test_index], k_y_valid.values, n_out=n_class)\n",
    "    #print('Test acc = %f\\n' % accuracy)\n",
    "    #print('Test f1 = %f\\n' % f1)\n",
    "    accs.append(accuracy)\n",
    "    f1s.append(f1)\n",
    "    cur_kfold += 1\n",
    "\n",
    "print('total acc scores is ', np.mean(accs))\n",
    "print('total f1 scores is ', np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total acc scores is  0.9460637713260814\n",
      "total f1 scores is  0.8732405856512834\n"
     ]
    }
   ],
   "source": [
    "#with CalibratedClassifierCV\n",
    "for train_index, test_index in kf.split(train[column]):\n",
    "    vec = TfidfVectorizer(ngram_range=(1,wins), min_df=3, max_df=0.9,use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    kfold_x_train = train[column][train_index]\n",
    "    kfold_x_valid = train[column][test_index]\n",
    "    k_y_train = (train['label']).astype(int)[train_index]\n",
    "    k_y_valid = (train['label']).astype(int)[test_index]\n",
    "\n",
    "    #print('获得tfidf特征')\n",
    "    k_trn_term_doc = vec.fit_transform(kfold_x_train)\n",
    "    k_test_term_doc = vec.transform(kfold_x_valid)\n",
    "    test_term_doc = vec.transform(test[column])\n",
    "\n",
    "    # 拟合数据\n",
    "    #print('拟合数据')\n",
    "    lin_clf = svm.LinearSVC()\n",
    "    lin_clf = CalibratedClassifierCV(lin_clf)\n",
    "    lin_clf.fit(k_trn_term_doc, k_y_train)\n",
    "\n",
    "    # 预测结果\n",
    "    #print('预测结果')\n",
    "    oof_predict[test_index] = lin_clf.predict_proba(k_test_term_doc)\n",
    "    predict += lin_clf.predict_proba(test_term_doc) / n_folds\n",
    "\n",
    "    # 计算准确度\n",
    "    accuracy = cal_acc(oof_predict[test_index], k_y_valid.values)\n",
    "    f1 = cal_f_alpha(oof_predict[test_index], k_y_valid.values, n_out=n_class)\n",
    "    #print('Test acc = %f\\n' % accuracy)\n",
    "    #print('Test f1 = %f\\n' % f1)\n",
    "    accs.append(accuracy)\n",
    "    f1s.append(f1)\n",
    "    cur_kfold += 1\n",
    "\n",
    "print('total acc scores is ', np.mean(accs))\n",
    "print('total f1 scores is ', np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
